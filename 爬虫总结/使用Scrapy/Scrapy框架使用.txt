关于Scrapy的基本原理介绍可以看下面两篇博客：
---------------------------------------------------------------
| https://blog.csdn.net/wickedvalley/article/details/51997360 |
| https://blog.csdn.net/junli_chen/article/details/48210061   |
------------------------------------------------------―――---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
安装Scrapy：

	1：安装 lxml ：pip install lxml
	 
	2：安装 pyOpenSSL ：pip install pyopenssl

	3：安装 Twisted ：pip install twisted （有可能失败）（附上python3.6版本的Twisted包（适用win32和win64））
	
	附上下载链接： https://pan.baidu.com/s/1sle2CmL
	
	4：安装 PyWin32 ：pip install pywin32

	5：安装 Scrapy ：pip install scrapy
		
	若下载失败，则可手动下载安装，下载的网站可使用国内镜像网站，具体可参考另一个文件："下载功能模块的国内网站.txt"

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		            以下都是在命令行中执行

创建Scrapy项目：scrapy startproject 项目名

创建Scrapy爬虫（进入项目中的spiders文件夹下）：scrapy genspider 爬虫名 爬取网站的主机名
（注：爬虫名必须唯一。爬取网站的主机名就是 "域名.com" 例：baidu.com/zhihu.com）

运行Scrapy爬虫：scrapy crawl 爬虫名

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Scrapy四部分

	1：Spider（主爬虫）（分析网页结构用）。爬虫名.py文件。
	2：DownloaderMiddleware（下载器中间件）（设置ip代理，cookies...所用）。middlewares.py中DownloaderMiddleware类。
	3：SpiderMiddleware（爬虫中间件）。middlewares.py中的SpiderMiddleware类。
	4：ItemPipelines（管道）（保存数据所用）。pipelines.py中的Pipeline类。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			     Scrapy项目下的各文件介绍

items.py（项）：该文件主要是定义要爬取数据的名字。（例如：小说名，图片地址，url链接....）

		定义格式为：name = scrapy.Field();

――――――――――――――――――――――――――――――――――――――――――――――
middlewares.py（中间件）：该文件主要用于设置访问时使用的Cookies，代理IP，User-Agent等。
		
		第一个类：DownloaderMiddleware()。各方法用法如下：

			process_request()：在发送请求前之前执行该方法（使用代理IP，Cookies，User-Agent，使用Selenium都在该方法中进行）。
				return：Request，None，Response，IgnoreRequest
					Request：一个全新的Request等待重新被调用
					None：返回None代表，继续执行优先级更低的Middleware，直到某个Middleware返回Response为止。
					Response：代表不执行优先级更低的middleware的process_request方法，而执行process_response方法。
					IgnoreRequest：更低优先级的middlewareprocess_exception方法会执行。

			process_response()：在把获得的请求发送给引擎之前执行该方法。（修改返回的请求中的一些数据，不常用）。return：Response，Request，IgnoreRequest
					Request：更低优先级的middleware不再调用，将该Request重新放入调度队列，相当于一个全新的Request。
					Response：执行更低优先级的middleware的process_response方法。
					IgnoreRequest：会回调Request的errorback方法，进行处理，若不处理，则忽略。
	
			process_exception()：发送请求发生错误时执行该方法。（可在该方法中对出错的请求进行一些操作）。

		第二个类：SpiderMiddleware()。scrapy的钩子框架

		有三个作用：（这里的Spider指的是当前爬虫的主文件，就是自己分析网页结构的类）
			1：在Downloadermiddleware生成的Response发送给Spider之前，对Response进行处理。	
			2：在Spider生成的Request发送给调度器之前，对Request进行处理。
			3：在Spider生成的item发送给pipeline之前对item进行处理。

		四个核心方法：
			1：process_spider_input(response,spider)：
				作用：当Response被SpiderMiddleware处理时调用。
				参数：Response对象。该Response对应的spider。
				return：None或抛出异常。
					1：返回None。更低优先级的SpiderMiddleware会被继续调用
					2：抛出异常。调用process_spider_exception方法来处理异常。
			
			2：process_spider_output(response,result,spider)：
				作用：当...input()或...exception()返回结果时，调用。
				参数：Response对象。包含Request或item的可迭代对象。哪个spider
				return：必须返回包含Request或item的可迭代对象。

			3：process_spider_exception(response,exception,spider)：
				作用：当process_spider_input抛出异常时调用。
				参数：抛出异常时的Response。Exception对象，被抛出的异常。哪个spider。
				return：None或一个包含Response或item的对象。
					1：返回None，其他的SpiderMiddleware的该方法会继续调用
					2：返回可迭代对象。其它的SpiderMiddleware的...output方法会被调用。
			4：process_start_requests(start_request,spider):
				作用：以Spider启动Request为参数进行调用。过程类似...output()。
				参数：爬虫启动时的Request，哪个爬虫。
				return：一个包含Request的可迭代对象。
――――――――――――――――――――――――――――――――――――――――――――――
pipelines.py（管道）：该文件主要用于对 item 携带的数据进行操作。（保存到数据库，下载图片等）
		
		process_item(item,spider)：该方法必须实现，也是在该方法中对数据进行操作。return item
		open_spider(spider)：爬虫启动时调用
		close_spider(spider)：爬虫停止时调用

――――――――――――――――――――――――――――――――――――――――――――――
settings.py（设置）：Scrapy爬虫中的各种配置都在该文件中设置

	一些配置简介：
		ITEM_PIPELINES = {"管道类的路径":优先级（数值越小，优先级越高）}
		
		DOWNLOADER_MIDDLEWARES = {"下载件的路径":"优先级"}

		.....

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			如何下载图片：

	1：可在 settings.py 中配置两个变量：
		IMAGES_STORE = "保存图片的文件夹名"
		IMAGE_URLS_FIELD = "item中保存图片连接的哪一项的名字"
	即可自动将图片下载在同一个文件夹内

	2：自定义图片下载
		
		(1)：自定义一个下载图片的管道类。（继承自 						  scrapy.pipelines.images.ImagesPipeline）

		(2)：若要指定下载图片的链接，可重写 get_media_requests() 方法：
			其中返回一个Request对象

		(3)：若要指定图片下载路径，可重写 file_path() 方法：
			其中返回一个完整的图片路径

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~	
设置User-Agent：
	1：修改settings.py文件里的“User-Agent”的值
	2：通过DownloaderMiddleware中的process_ware方法修改，方法如下：
		<1>：在middlewares.py文件中，随便添加一个用于设置User-Agent的类。	  
		<2>：实现process_request方法，在其中设置User-Agent（跟requests包设置方法一样）
		<3>：将自定义的设置User-Agent的类，在settings.py文件中的“DOWNLOADER_MIDDLEWARES”中声明一下。

使用代理ip：
	request.meta["proxy"] = ip;

==========================================================================================
异步写入数据库：
	在使用Scrapy爬取数据时，由于scrapy爬取的速度非常快，但写入数据库的步骤去非常慢，这是就需要使用异步写入数据库了。

使用方法：
	首先需要下载Twisted模块，该模块使用 pip 不一定能下载成功，若下载失败可使用下方连接进行下载，然后手动安装：
	
	下载地址：http://mirrors.aliyun.com/pypi/simple/twisted/	

	pip install Twisted文件的路径	

------------------------------------------------------------------------------------------
	from twisted.enterprise import adbapi; # 该模块使用来异步操作数据库的
	from pymysql.cursors import DictCursor; # 查询数据时，以字典形式返回数据
	
	#  连接数据库所需的各项配置
	db_settings = {host, port, user, password, db....,"cursor":DictCursor}; # 最后一个一定要写，它指定了操作数据库所使用的游标。
	
	# 生成一个数据库连接池
	db_pool = adbapi.ConnectionPool("mysql",**db_settings);

以上步骤可以在 __init__() 方法中完成。
以下步骤可以在 process_item()方法中完成。
	
	# 开始插入数据
	res = db_pool.runInteraction(插入数据的方法, 要插入的数据); # 插入数据的方法中必须要有两个参数：
	1：游标：执行插入数据操作需要用到，runInteraction() 方法会自动传入上方定义的游标。
	2：数据：要写入数据库的数据
	
	#  当插入数据出现错误时，执行该语句
	res.addErrback(处理错误的方法, 发生错误的数据); 

	# 处理错误的方法中必须要有两个参数：
		1：错误的原因：由 addErrback() 自动传入
		2：数据：发生错误的数据
新建一个插入数据的方法。
例：
	    def insert_into(self,cursor,item):
        	sql = '...............';
        	cursor.execute(sql);
新建一个输出错误的方法。
例：
	    def handle_error(self,failure,item,spider):
        	print(failure)